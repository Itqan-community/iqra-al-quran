# ------------- Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø­Ø³Ù‘Ù† Ø¨Ù€ faster-whisper -------------
# Use the optimized CTranslate2 model for 4x speed improvement
MODEL_NAME = "OdyAsh/faster-whisper-base-ar-quran"

# Import faster-whisper for optimal performance
try:
    from faster_whisper import WhisperModel
    USE_FASTER_WHISPER = True
    print("âœ… Using faster-whisper for OPTIMAL performance (4x faster!)")
except ImportError:
    from transformers import pipeline
    USE_FASTER_WHISPER = False
    print("âš ï¸ faster-whisper not available, using transformers (slower)")

# Device configuration
import torch
gpu_available = torch.cuda.is_available()

# Global model instance
asr_model = None

def get_asr_pipeline():
    """ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ASR Ù…Ø­Ø³Ù‘Ù† Ø¨Ù€ faster-whisper Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£ÙØ¶Ù„ Ø£Ø¯Ø§Ø¡"""
    global asr_model, MODEL_NAME
    
    if asr_model is None:
        print(f"ğŸš€ Loading OPTIMIZED ASR model: {MODEL_NAME}")
        device_info = 'GPU' if gpu_available else 'CPU'
        print(f"ğŸ¯ Device: {device_info}")
        
        start_time = time.time()
        
        try:
            if USE_FASTER_WHISPER:
                # Use faster-whisper (CTranslate2) - Much faster!
                device = "cuda" if gpu_available else "cpu"
                compute_type = "float16" if gpu_available else "int8"  # Quantization for speed
                
                print(f"ğŸ”¥ Loading with faster-whisper (device={device}, compute_type={compute_type})")
                asr_model = WhisperModel(
                    MODEL_NAME,
                    device=device,
                    compute_type=compute_type,
                    download_root=None,
                    local_files_only=False
                )
                
                load_time = time.time() - start_time
                print(f"âœ… faster-whisper model loaded in {load_time:.2f} seconds")
                
                # Model warmup with faster-whisper
                print("ğŸ”¥ Warming up faster-whisper model...")
                warmup_start = time.time()
                try:
                    import numpy as np
                    # Create 1 second of silence for warmup
                    dummy_audio = np.zeros(16000, dtype=np.float32)
                    
                    # Run warmup inference
                    segments, info = asr_model.transcribe(dummy_audio, language="ar")
                    list(segments)  # Consume the generator
                    
                    warmup_time = time.time() - warmup_start
                    print(f"ğŸš€ faster-whisper warmed up in {warmup_time:.2f} seconds!")
                    print("ğŸ’¡ Model ready for BLAZING FAST inference!")
                    
                except Exception as e:
                    print(f"âš ï¸ faster-whisper warmup failed: {e}")
                
            else:
                # Fallback to transformers pipeline
                device = 0 if gpu_available else -1
                asr_model = pipeline(
                    "automatic-speech-recognition",
                    model="tarteel-ai/whisper-base-ar-quran",
                    device=device,
                    torch_dtype=torch.float16 if gpu_available else torch.float32
                )
                
                load_time = time.time() - start_time
                print(f"âœ… Transformers pipeline loaded in {load_time:.2f} seconds")
                
        except Exception as e:
            print(f"âŒ Error loading optimized model: {e}")
            print("ğŸ”„ Falling back to basic model...")
            
            # Ultimate fallback
            if USE_FASTER_WHISPER:
                try:
                    asr_model = WhisperModel("base", device="cpu", compute_type="int8")
                    print("âœ… Fallback faster-whisper model loaded")
                except:
                    USE_FASTER_WHISPER = False
            
            if not USE_FASTER_WHISPER:
                device = 0 if gpu_available else -1
                from transformers import pipeline
                asr_model = pipeline("automatic-speech-recognition", model="openai/whisper-base", device=device)
                print("âœ… Ultimate fallback model loaded")
    
    return asr_model

def transcribe_audio_optimized(audio_path):
    """Transcribe audio using the optimized faster-whisper model"""
    model = get_asr_pipeline()
    
    if USE_FASTER_WHISPER:
        # Use faster-whisper transcription (MUCH FASTER!)
        segments, info = model.transcribe(audio_path, language="ar")
        
        # Extract text from segments
        transcription = ""
        for segment in segments:
            transcription += segment.text + " "
        
        return {
            "text": transcription.strip(),
            "language": info.language,
            "language_probability": info.language_probability,
            "duration": info.duration,
            "optimized": True
        }
    else:
        # Use transformers pipeline (fallback)
        import numpy as np
        import soundfile as sf
        
        if isinstance(audio_path, str):
            # Load audio file as numpy array
            audio, sample_rate = sf.read(audio_path)
            if sample_rate != 16000:
                import librosa
                audio = librosa.resample(audio, orig_sr=sample_rate, target_sr=16000)
        else:
            audio = audio_path
            
        result = model(audio)
        return {"text": result["text"], "optimized": False}
